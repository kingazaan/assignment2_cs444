{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWMWW8Ab_345"
   },
   "source": [
    "# (Optional) Colab Setup\n",
    "If you aren't using Colab, you can delete the following code cell. This is just to help students with mounting to Google Drive to access the other .py files and downloading the data, which is a little trickier on Colab than on your local machine using Jupyter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "vH4wc4iD_6w_"
   },
   "outputs": [],
   "source": [
    "# # you will be prompted with a window asking to grant permissions\n",
    "# from google.colab import drive\n",
    "# drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "XpNsPHZc_879"
   },
   "outputs": [],
   "source": [
    "# TODO: fill in the path in your Google Drive in the string below\n",
    "# Note: do not escape slashes or spaces in the path string\n",
    "import os\n",
    "datadir = \"C:\\\\Users\\\\azaan\\\\OneDrive\\\\Desktop\\\\Deep Learning for Computer Vision HW\\\\assignment2_cs444\"\n",
    "if not os.path.exists(datadir):\n",
    "  !ln -s \"/content/drive/My Drive/path/to/your/assignment2/\" $datadir\n",
    "os.chdir(datadir)\n",
    "# !pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cHqo6b1_Bzk"
   },
   "source": [
    "# Implement a Neural Network\n",
    "\n",
    "This notebook contains testing code to help you develop a neural network by implementing the forward pass and backpropagation algorithm in the `models/neural_net.py` file. \n",
    "\n",
    "You will implement your network in the class `NeuralNetwork` inside the file `models/neural_net.py` to represent instances of the network. The network parameters are stored in the instance variable `self.params` where keys are string parameter names and values are numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "nTt_CiWh_Bzm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# from models.neural_net import NeuralNetwork\n",
    "# Note: neural_net_solution folder does not exist. This is supposed to be a copy of the models/neural_net.py. This may help you to verify your implementation.\n",
    "from neural_net_solution import NeuralNetwork\n",
    "\n",
    "\n",
    "# For auto-reloading external modules\n",
    "# See http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\"Returns relative error\"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5X9DO-5_Bzn"
   },
   "source": [
    "The cell below initializes a toy dataset and corresponding model which will allow you to check your forward and backward pass by using a numeric gradient check. Note that we set a random seed for repeatable experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "358jAXcc_Bzn"
   },
   "outputs": [],
   "source": [
    "input_size = 2\n",
    "hidden_size = 10\n",
    "num_classes = 3\n",
    "num_inputs = 5\n",
    "optimizer = 'SGD'\n",
    "\n",
    "\n",
    "def init_toy_model(num_layers):\n",
    "    \"\"\"Initializes a toy model\"\"\"\n",
    "    np.random.seed(0)\n",
    "    hidden_sizes = [hidden_size] * (num_layers - 1)\n",
    "    return NeuralNetwork(input_size, hidden_sizes, num_classes, num_layers, optimizer)\n",
    "\n",
    "def init_toy_data():\n",
    "    \"\"\"Initializes a toy dataset\"\"\"\n",
    "    np.random.seed(0)\n",
    "    X = np.random.randn(num_inputs, input_size)\n",
    "    y = np.random.randn(num_inputs, num_classes)\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zh_v9biP_Bzn"
   },
   "source": [
    "# Implement forward and backward pass\n",
    "\n",
    "The first thing you will do is implement the forward pass of your neural network. The forward pass should be implemented in the `forward` function. You can use helper functions like `linear`, `relu`, and `sigmoid` to help organize your code.\n",
    "\n",
    "Next, you will implement the backward pass using the backpropagation algorithm. Backpropagation will compute the gradient of the loss with respect to the model parameters `W1`, `b1`, ... etc. Use a sigmoid fuction with mse loss for loss calcuation. Fill in the code blocks in `NeuralNetwork.backward`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GjAwpT2z_Bzo"
   },
   "source": [
    "# Gradient  check\n",
    "\n",
    "If you have implemented your forward pass through the network correctly, you can use the following cell to debug your backward pass with a numeric gradient check. This function assumes that your gradient is divided by y.shape[0] * y.shape[1], where y is the first input to the backward function. You should use mse loss after the sigmoid layer for this gradient check. If your backward pass has been implemented correctly, the max relative error between your analytic solution and the numeric solution should be around 1e-7 or less for all parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "UZM47qUP_Bzo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of weights [(2, 10), (10, 3)]\n",
      "shape of all inputs 2 [10] 3 2\n",
      "[10]\n",
      "2\n",
      "(2, 10) (10,)\n",
      "one layer done\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'relu_outputs_2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [48], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m num \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m]:\n\u001b[0;32m     13\u001b[0m     net \u001b[38;5;241m=\u001b[39m init_toy_model(num)\n\u001b[1;32m---> 14\u001b[0m     \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     net\u001b[38;5;241m.\u001b[39mbackward(y)\n\u001b[0;32m     16\u001b[0m     gradients \u001b[38;5;241m=\u001b[39m deepcopy(net\u001b[38;5;241m.\u001b[39mgradients)\n",
      "File \u001b[1;32mc:\\Users\\azaan\\OneDrive\\Desktop\\Deep Learning for Computer Vision HW\\assignment2_cs444\\neural_net_solution.py:174\u001b[0m, in \u001b[0;36mNeuralNetwork.forward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mone layer done\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# self.outputs[\"linear_outputs_2_\" + str(i)] = self.linear(W, self.outputs[\"relu_outputs_\" + str(i)], b)\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# self.outputs[\"sigmoid_outputs_\" + str(i)] = self.sigmoid(self.outputs[\"linear_outputs_2_\" + str(i)])\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# print(self.num_layers)\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;66;03m# print(self.params.keys())\u001b[39;00m\n\u001b[1;32m--> 174\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear_outputs_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers)], \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrelu_outputs_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers)])\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_layer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear_outputs_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_layer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'relu_outputs_2'"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "from utils.gradient_check import eval_numerical_gradient\n",
    "\n",
    "X, y = init_toy_data()\n",
    "\n",
    "\n",
    "def f(W):\n",
    "    net.forward(X)\n",
    "    return net.backward(y)\n",
    "\n",
    "for num in [2, 3]:\n",
    "    net = init_toy_model(num)\n",
    "    net.forward(X)\n",
    "    net.backward(y)\n",
    "    gradients = deepcopy(net.gradients)\n",
    "\n",
    "    for param_name in net.params:\n",
    "        param_grad_num = eval_numerical_gradient(f, net.params[param_name], verbose=False)\n",
    "        print('%s max relative error: %e' % (param_name, rel_error(param_grad_num, gradients[param_name])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
